{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariabarbosa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           #AUTHID                                               TEXT cEXT  \\\n",
      "0  2000_576170.txt  I just got back from your class, so I decided ...    n   \n",
      "1  2000_576862.txt  It is 9:35 and I am beginning my stream of con...    y   \n",
      "2  1998_733941.txt  Not only was the server down but it has taken ...    y   \n",
      "3  2000_904579.txt  I am not exactly sure how this is supposed to ...    y   \n",
      "4  2002_097387.txt  Well, here I am on Friday, September something...    n   \n",
      "\n",
      "  cNEU cAGR cCON cOPN  split  \n",
      "0    n    y    y    n      2  \n",
      "1    n    y    n    y      3  \n",
      "2    n    y    y    y      3  \n",
      "3    n    y    n    n      0  \n",
      "4    n    y    n    n      9  \n"
     ]
    }
   ],
   "source": [
    "## read csv essay datset to a dataframe with pandas \n",
    "df = pd.read_csv('essay.csv', index_col=False,sep=',')\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update classifiers to integer value\n",
    "df['cNEU'] = df['cNEU'].map({'n': 0, 'y': 1})\n",
    "df['cAGR'] = df['cAGR'].map({'n': 0, 'y': 1})\n",
    "df['cCON'] = df['cCON'].map({'n': 0, 'y': 1})\n",
    "df['cOPN'] = df['cOPN'].map({'n': 0, 'y': 1})\n",
    "df['cEXT'] = df['cEXT'].map({'n': 0, 'y': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize, remove stop words, standardize in lowercase, extract adjectives, verbs and noun\n",
    "def dataPrep(text):\n",
    "    doc = nlp(text)  \n",
    "    res = \" \".join([token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and (token.pos_ == \"VERB\" or token.pos_ == \"ADJ\" or token.pos_ == \"NOUN\")])   \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_7979/2369796708.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TEXT'][i] = dataPrep(df['TEXT'][i])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(df)):\n",
    "    df['TEXT'][i] = dataPrep(df['TEXT'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Compute TF-IDF values\n",
    "Returns the words with tf_idf value > 0\n",
    "'''\n",
    "def tf_idf (big5):\n",
    "   \n",
    "    text = df[df[big5]==1].TEXT\n",
    "\n",
    "    count = CountVectorizer()\n",
    "    word_count=count.fit_transform(text)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count)\n",
    "\n",
    "    tf_idf_vector=tfidf_transformer.transform(word_count)\n",
    "    feature_names = count.get_feature_names()\n",
    "\n",
    "    first_document_vector=tf_idf_vector[1]\n",
    "    df_tfifd= pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "\n",
    "    dataframe = df_tfifd.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "    dataframe['feature_names'] = dataframe.index\n",
    "    dataframe = dataframe.drop(dataframe[dataframe.tfidf == 0].index)\n",
    "    dataframe = dataframe.drop(dataframe[dataframe.feature_names == 'bps'].index)\n",
    "\n",
    "    weights = {}\n",
    "    for row in dataframe.itertuples():\n",
    "        weights [row.feature_names] =row.tfidf\n",
    "\n",
    "    return weights, (list(set(dataframe.feature_names)))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wN,features_names_NEU =tf_idf ('cNEU')\n",
    "wA,features_names_AGR = tf_idf ('cAGR')\n",
    "wC,features_names_CON = tf_idf ('cCON')\n",
    "wO,features_names_OPN =tf_idf ('cOPN')\n",
    "wE,features_names_EXT = tf_idf ('cEXT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous results was possible to distinguish agreeableness and neu-\n",
    "roticism with almost disjoint word sets. However, the word-set for extroversion,\n",
    "openness and conscientiousness are overlapping (despite being almost disjoint\n",
    "with the other two).\n",
    "\n",
    "Adapting from the Personality Adaptations\n",
    "theory, were identify three primary processes: *paranoid*, *schizoid* and *neuroticism*. Focusing in this 3 mental process we obtain a initial lexicon composed by 228 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paranoid:  87 Schizoid:  53 Neuroticism:  136\n",
      "Initial lexicon number of words:  276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_7979/786899918.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lexicon_A = lexicon_A.append(lexicon_O)\n",
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_7979/786899918.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lexicon_A = lexicon_A.append(lexicon_N)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# s√£o 9917  entradas\n",
    "\n",
    "text = []\n",
    "words_lexico = features_names_AGR + features_names_OPN + features_names_NEU\n",
    "print('Paranoid: ', len(features_names_AGR), 'Schizoid: ', len(features_names_OPN), 'Neuroticism: ', len(features_names_NEU))\n",
    "print('Initial lexicon number of words: ', len(words_lexico))\n",
    "\n",
    "\n",
    "## Creat a csv file with the initial lexicon\n",
    "\n",
    "lexicon_A = pd.DataFrame({\"word\": wA.keys(),\n",
    "                         \"tf_idf\": wA.values(),\n",
    "                         'classification':'Paranoid'})\n",
    "\n",
    "lexicon_O = pd.DataFrame({\"word\": wO.keys(),\n",
    "                         \"tf_idf\": wO.values(),\n",
    "                         'classification':'Schizoid'})\n",
    "\n",
    "lexicon_N = pd.DataFrame({\"word\": wN.keys(),\n",
    "                         \"tf_idf\": wN.values(),\n",
    "                         'classification':'Neuroticism'})\n",
    "\n",
    "lexicon_A = lexicon_A.append(lexicon_O)\n",
    "lexicon_A = lexicon_A.append(lexicon_N)\n",
    "\n",
    "\n",
    "\n",
    "lexicon_A.to_csv('MentaLex_initial.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase word-set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "def synonyms (list_words):\n",
    "\n",
    "    synonyms_traits  = []\n",
    "    for word in list_words:\n",
    "        synonyms = []\n",
    "\n",
    "        for syn in wn.synsets(word):\n",
    "                for l in syn.lemmas():\n",
    "                    doc = nlp(l.name())\n",
    "                    s = [ str(token) for token in doc if (token.pos_ == \"VERB\" or token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\")] \n",
    "                    synonyms = synonyms + s\n",
    "        synonyms_traits = synonyms_traits + synonyms\n",
    "                    \n",
    "    # print(set(synonyms_traits))\n",
    "\n",
    "    syn_final_list =  set (list_words + synonyms_traits)\n",
    "    return syn_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schizoid:  815 ; paranoid:  1090 ; Neuroticism:  1527\n",
      "number of words with synonyms  3432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "O_syn = synonyms (features_names_OPN)\n",
    "A_syn =synonyms (features_names_AGR)\n",
    "N_syn =synonyms (features_names_NEU)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('schizoid: ', len(O_syn), '; paranoid: ' , len(A_syn),'; Neuroticism: ', len(N_syn))\n",
    "\n",
    "length = len(list(O_syn) + list(A_syn) + list(N_syn))\n",
    "\n",
    "#tamanho_sem_repetidos = len (set(list(O_syn) +  list(A_syn) + list(N_syn)))\n",
    "\n",
    "print('number of words with synonyms ', length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the synonyms words in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_7979/1211016626.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lexicon_A = lexicon_A.append(lexicon_O)\n",
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_7979/1211016626.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lexicon_A = lexicon_A.append(lexicon_N)\n"
     ]
    }
   ],
   "source": [
    "## Creat a csv file with the initial lexicon\n",
    "\n",
    "lexicon_A = pd.DataFrame({\"word\": list(A_syn),\n",
    "                         'classification':'Paranoid'})\n",
    "\n",
    "lexicon_O = pd.DataFrame({\"word\": list(O_syn),\n",
    "                         'classification':'Schizoid'})\n",
    "\n",
    "lexicon_N = pd.DataFrame({\"word\": list(N_syn),\n",
    "                         'classification':'Neuroticism'})\n",
    "\n",
    "lexicon_A = lexicon_A.append(lexicon_O)\n",
    "lexicon_A = lexicon_A.append(lexicon_N)\n",
    "\n",
    "\n",
    "\n",
    "lexicon_A.to_csv('MentaLex_synonyms.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lexicon coverage in twitter personality dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_1162/1720765092.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_twitter['STATUS'][i] = dataPrep(data_twitter['STATUS'][i])\n"
     ]
    }
   ],
   "source": [
    "## Prepare twitter dataset\n",
    "data_twitter = pd.read_csv('mypersonality.csv', index_col=False,sep=',', encoding='ISO 8859-1')\n",
    "\n",
    "\n",
    "data_twitter['cNEU'] = data_twitter['cNEU'].map({'n': 0, 'y': 1})\n",
    "data_twitter['cAGR'] = data_twitter['cAGR'].map({'n': 0, 'y': 1})\n",
    "data_twitter['cCON'] = data_twitter['cCON'].map({'n': 0, 'y': 1})\n",
    "data_twitter['cOPN'] = data_twitter['cOPN'].map({'n': 0, 'y': 1})\n",
    "data_twitter['cEXT'] = data_twitter['cEXT'].map({'n': 0, 'y': 1})\n",
    "\n",
    "for i in range(len(data_twitter)):\n",
    "    data_twitter['STATUS'][i] = dataPrep(data_twitter['STATUS'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial lexicon (276 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of initial lexicon words in twitter dataset (%) is  0.6944640516285167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "words_lexicon = features_names_AGR + features_names_OPN + features_names_NEU\n",
    "c = 0\n",
    "for row in data_twitter.itertuples():\n",
    "        words_twitter = row.STATUS.split()\n",
    "        for w in words_twitter:\n",
    "                if w in words_lexicon:\n",
    "                        c = c + 1\n",
    "                        break\n",
    "        \n",
    "\n",
    "print('Coverage of initial lexicon words in twitter dataset (%) is ' ,c/len(data_twitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3432\n",
      "Coverage of lexicon words in twitter dataset (%) is  0.8109307250176465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_lexico = list(O_syn) + list(A_syn) + list(N_syn) #list(O_syn) + list(C_syn) + list(E_syn) + list(A_syn) + list(N_syn)\n",
    "print(len(words_lexico))\n",
    "c = 0\n",
    "for row in data_twitter.itertuples():\n",
    "        words_twitter = row.STATUS.split()\n",
    "        for w in words_twitter:\n",
    "                if w in words_lexico:\n",
    "                        c = c + 1\n",
    "                        break\n",
    "        \n",
    "\n",
    "\n",
    "print('Coverage of lexicon words in twitter dataset (%) is ' ,c/len(data_twitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding words analysis\n",
    "\n",
    "\n",
    "In this section we pretend to analyse the words to avoid for every mental process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf_idf (big5):\n",
    "    \n",
    "    text = df[df[big5]==0].TEXT  ## This line is diferent from the previous function. In this case we are intersted in the texts not writted by bif5 trait\n",
    "\n",
    "   \n",
    "    count = CountVectorizer()\n",
    "    word_count=count.fit_transform(text)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count)\n",
    "   # df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "   # df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "\n",
    "\n",
    "    tf_idf_vector=tfidf_transformer.transform(word_count)\n",
    "    feature_names = count.get_feature_names()\n",
    "\n",
    "    first_document_vector=tf_idf_vector[1]\n",
    "    df_tfifd= pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "\n",
    "    dataframe = df_tfifd.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "    dataframe['feature_names'] = dataframe.index\n",
    "    dataframe = dataframe.drop(dataframe[dataframe.tfidf == 0].index)\n",
    "    dataframe = dataframe.drop(dataframe[dataframe.feature_names == 'bps'].index)\n",
    "    #print(dataframe)  # list(dataframe.features_names)\n",
    "    \n",
    "    weights = {}\n",
    "    for row in dataframe.itertuples():\n",
    "        weights [row.feature_names] =row.tfidf\n",
    "\n",
    "\n",
    "\n",
    "    f = (list(dataframe.feature_names))\n",
    "\n",
    "    #dataframe.to_csv(big5 +'.verbs_adj_noun.csv', index=False)\n",
    "\n",
    "    return weights,f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Obter word\n",
    "\n",
    "wN, features_COM_NEU =tf_idf ('cNEU')\n",
    "wA, features_COM_AGR = tf_idf ('cAGR')\n",
    "#f1 = tf_idf ('cCON')\n",
    "\n",
    "wO,features_COM_OPN =tf_idf ('cOPN')\n",
    "#tf_idf ('cEXT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paranoid:  85 Schizoid:  144 Neuroticism:  87\n",
      "Initial lexicon number of words:  316\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "words_lexico = features_COM_AGR + features_COM_OPN + features_COM_NEU\n",
    "print('Paranoid: ', len(features_COM_AGR), 'Schizoid: ', len(features_COM_OPN), 'Neuroticism: ', len(features_COM_NEU))\n",
    "print('Initial lexicon number of words: ', len(words_lexico))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save in to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lexicon_avoiding_A = pd.DataFrame({\"word\": wA.keys(),\n",
    "                         \"tf_idf\": wA.values(),\n",
    "                         'Avoiding_IN':'Paranoid'})\n",
    "\n",
    "lexicon_avoiding_O = pd.DataFrame({\"word\": wO.keys(),\n",
    "                         \"tf_idf\": wO.values(),\n",
    "                         'Avoiding_IN':'Schizoid'})\n",
    "\n",
    "lexicon_avoiding_N = pd.DataFrame({\"word\": wN.keys(),\n",
    "                         \"tf_idf\": wN.values(),\n",
    "                         'Avoiding_IN':'Neuroticism'})\n",
    "\n",
    "lexicon_avoiding_A = lexicon_avoiding_A.append(lexicon_avoiding_O)\n",
    "lexicon_avoiding_A = lexicon_avoiding_A.append(lexicon_avoiding_N)\n",
    "\n",
    "len(lexicon_avoiding_A)\n",
    "\n",
    "lexicon_avoiding_A.to_csv('MentaLex_avoidingWords_initial.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the lexicon for the words to avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "def synonyms (list_words):\n",
    "\n",
    "    synonyms_traits  = []\n",
    "    for word in list_words:\n",
    "        synonyms = []\n",
    "\n",
    "        for syn in wn.synsets(word):\n",
    "                for l in syn.lemmas():\n",
    "                    doc = nlp(l.name())\n",
    "                    s = [str(token) for token in doc if (token.pos_ == \"VERB\" or token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\")] \n",
    "                    synonyms = synonyms + s\n",
    "        synonyms_traits = synonyms_traits + synonyms\n",
    "                    \n",
    "    # print(set(synonyms_traits))\n",
    "\n",
    "    syn_final_list =  list(set (list_words + synonyms_traits))\n",
    "    return syn_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schizoid:  1698 ; paranoid:  851 ; Neuroticism:  1090\n",
      "number of words with synonyms  3639\n"
     ]
    }
   ],
   "source": [
    "O_syn_COM = synonyms (features_COM_OPN)\n",
    "A_syn_COM =synonyms (features_COM_AGR)\n",
    "N_syn_COM =synonyms (features_COM_NEU)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('schizoid: ', len(O_syn_COM), '; paranoid: ' , len(A_syn_COM),'; Neuroticism: ', len(N_syn_COM))\n",
    "\n",
    "length = len(list(O_syn_COM) + list(A_syn_COM) + list(N_syn_COM))\n",
    "\n",
    "#tamanho_sem_repetidos = len (set(list(O_syn) +  list(A_syn) + list(N_syn)))\n",
    "\n",
    "print('number of words with synonyms ', length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_7979/1695439807.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lexicon_avoiding_A = lexicon_avoiding_A.append(lexicon_avoiding_O)\n",
      "/var/folders/7v/nzkm4hw14qv5j4fghjbfccgr0000gn/T/ipykernel_7979/1695439807.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lexicon_avoiding_A = lexicon_avoiding_A.append(lexicon_avoiding_N)\n"
     ]
    }
   ],
   "source": [
    "## In this case is not possible to obtain the tf_idf value, since the words are synonyms\n",
    "\n",
    "\n",
    "lexicon_avoiding_A = pd.DataFrame({\"word\": A_syn_COM,\n",
    "                         'Avoiding_IN':'Paranoid'})\n",
    "\n",
    "lexicon_avoiding_O = pd.DataFrame({\"word\": O_syn_COM,\n",
    "                         'Avoiding_IN':'Schizoid'})\n",
    "\n",
    "lexicon_avoiding_N = pd.DataFrame({\"word\": N_syn_COM,\n",
    "                         'Avoiding_IN':'Neuroticism'})\n",
    "\n",
    "lexicon_avoiding_A = lexicon_avoiding_A.append(lexicon_avoiding_O)\n",
    "lexicon_avoiding_A = lexicon_avoiding_A.append(lexicon_avoiding_N)\n",
    "\n",
    "\n",
    "lexicon_avoiding_A.to_csv('MentaLex_avoidingWords_synonyms.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e73066c94f8e0a03dc010f90f9bfe8189274fe2970ef1c1470299afeb680a60a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
